{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-header",
   "metadata": {},
   "source": [
    "# Module 16: Week 5 Analysis - Advanced Neural Network Optimization\n",
    "\n",
    "This notebook documents the strategy, analysis, and query generation for **Week 5** (Module 16).\n",
    "\n",
    "## Focus\n",
    "- **Deep Learning Concepts**: Applying insights from AlexNet, ImageNet, and modern architectures\n",
    "- **Enhanced Neural Network Surrogates**: Building on Week 4's NN approach with architectural improvements\n",
    "- **Framework Parallels**: Drawing connections between PyTorch/TensorFlow design and BBO strategy\n",
    "- **Feature Hierarchies**: Understanding how learned representations guide optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "module16-concepts",
   "metadata": {},
   "source": [
    "## Module 16 Concepts Applied to BBO\n",
    "\n",
    "### Key Deep Learning Insights\n",
    "\n",
    "**1. AlexNet's Breakthrough (2012)**\n",
    "- Demonstrated that **depth + data + compute** can solve complex problems\n",
    "- Dropout regularization prevents overfitting with limited data\n",
    "- ReLU activations enable efficient gradient flow\n",
    "\n",
    "**Application to BBO**: Our ensemble of MLPs uses dropout and ReLU, inspired by AlexNet's success.\n",
    "\n",
    "**2. Five Building Blocks of Deep Learning**\n",
    "1. **Linear layers**: Transform input dimensions\n",
    "2. **Activations**: Introduce non-linearity (ReLU, sigmoid)\n",
    "3. **Normalization**: Stabilize training (BatchNorm, LayerNorm)\n",
    "4. **Regularization**: Prevent overfitting (dropout, weight decay)\n",
    "5. **Loss functions**: Define the optimization objective\n",
    "\n",
    "**Application to BBO**: All five blocks are present in our surrogate architecture.\n",
    "\n",
    "**3. Framework Choice (PyTorch vs TensorFlow)**\n",
    "- PyTorch: Dynamic graphs, easier debugging, research-friendly\n",
    "- TensorFlow: Production-ready, distributed training, deployment\n",
    "\n",
    "**Application to BBO**: We use PyTorch for its dynamic gradient computation, essential for computing âˆ‚y/âˆ‚x at arbitrary points.\n",
    "\n",
    "**4. Feature Hierarchies**\n",
    "- Early layers: Detect simple patterns (edges, textures)\n",
    "- Deeper layers: Combine into complex features\n",
    "- Final layers: Task-specific representations\n",
    "\n",
    "**Application to BBO**: Our 2-layer MLP learns which input dimensions matter (feature hierarchy) for predicting function output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import load_data, save_submission\n",
    "import warnings\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# sklearn for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Week 5 BBO Analysis - Module 16: Advanced Neural Networks - Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "week4-results",
   "metadata": {},
   "source": [
    "## Week 4 Results Analysis\n",
    "\n",
    "| Function | Week 3 | Week 4 | Best Ever | Change | Notes |\n",
    "|----------|--------|--------|-----------|--------|-------|\n",
    "| 1 | ~0 | **0.196** | **0.196 (W4)** | ðŸš€ | **BREAKTHROUGH!** First non-zero value! |\n",
    "| 2 | 0.345 | **0.667** | **0.667 (W4)** | ðŸš€ | **NEW BEST!** Beat initial (0.611) |\n",
    "| 3 | -0.041 | -0.085 | -0.035 (init) | â†˜ | Slight regression |\n",
    "| 4 | -1.27 | -24.52 | 0.600 (W1) | â†˜ | Exploration failed |\n",
    "| 5 | 1509.0 | 1289.3 | 1618.5 (W1) | â†˜ | Below best |\n",
    "| 6 | -0.769 | **-0.717** | **-0.714 (init)** | â†— | Near best! |\n",
    "| 7 | 2.229 | **2.396** | **2.396 (W4)** | ðŸš€ | **NEW BEST!** |\n",
    "| 8 | 9.915 | 9.877 | 9.915 (W3) | â†˜ | Slight regression |\n",
    "\n",
    "### Key Insights for Week 5\n",
    "\n",
    "**Major Successes (ðŸš€)**:\n",
    "- **F1**: Grid search at [0.65, 0.65] found the first non-zero value (0.196)! This is huge.\n",
    "- **F2**: Returning to best initial point yielded 0.667 - even better than initial 0.611!\n",
    "- **F7**: Micro-perturbation improved from 2.29 to 2.396 - new global best!\n",
    "\n",
    "**Lessons Learned**:\n",
    "- **F4 Exploration Failed**: Moving to [0.75, 0.75, 0.75, 0.75] gave -24.52. The positive region is NOT in the high corner.\n",
    "- **F5 Gradient Over-corrected**: Moving away from Week 1's best reduced performance.\n",
    "- **F6 Near Optimal**: Returning to initial gave -0.717, very close to best (-0.714).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSurrogateNN(nn.Module):\n",
    "    \"\"\"Enhanced Neural Network surrogate with Module 16 concepts.\n",
    "    \n",
    "    Improvements inspired by AlexNet and modern architectures:\n",
    "    - Deeper network option for complex functions\n",
    "    - Skip connections for gradient flow (ResNet-inspired)\n",
    "    - Better initialization (Xavier/He)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.1, use_skip=False):\n",
    "        super(EnhancedSurrogateNN, self).__init__()\n",
    "        \n",
    "        self.use_skip = use_skip and (input_dim == hidden_dims[-1] if hidden_dims else False)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),  # Stabilizes training\n",
    "                nn.ReLU(),                  # AlexNet's key activation\n",
    "                nn.Dropout(dropout)         # Regularization\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Xavier initialization (better for ReLU)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class ImprovedEnsembleSurrogate:\n",
    "    \"\"\"Ensemble of NNs with improved training inspired by Module 16.\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Cosine annealing LR schedule (like modern vision models)\n",
    "    2. Gradient clipping for stability\n",
    "    3. Early stopping with patience\n",
    "    4. Diverse architectures in ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, n_samples, n_models=5):\n",
    "        self.input_dim = input_dim\n",
    "        self.n_models = n_models\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.schedulers = []\n",
    "        self.scalers = {'X': StandardScaler(), 'y': StandardScaler()}\n",
    "        \n",
    "        # Adaptive architecture based on data size\n",
    "        base_hidden = max(16, min(64, n_samples * 2))\n",
    "        \n",
    "        # Diverse architectures for ensemble (like ensemble methods in competitions)\n",
    "        architectures = [\n",
    "            [base_hidden, base_hidden // 2],                    \n",
    "            [base_hidden * 2, base_hidden, base_hidden // 2],   \n",
    "            [base_hidden, base_hidden],                          \n",
    "            [base_hidden // 2, base_hidden // 4],               \n",
    "            [base_hidden, base_hidden, base_hidden // 2],       \n",
    "        ]\n",
    "        \n",
    "        dropout = min(0.3, max(0.1, 1.0 / np.sqrt(n_samples)))\n",
    "        \n",
    "        for i in range(n_models):\n",
    "            torch.manual_seed(42 + i * 7)\n",
    "            arch = architectures[i % len(architectures)]\n",
    "            model_dropout = dropout * (0.8 + 0.4 * np.random.random())\n",
    "            \n",
    "            model = EnhancedSurrogateNN(input_dim, arch, model_dropout).to(device)\n",
    "            \n",
    "            lr = 0.01 / (1 + len(arch) * 0.2)\n",
    "            weight_decay = 1e-3 / n_samples\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-5)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            self.optimizers.append(optimizer)\n",
    "            self.schedulers.append(scheduler)\n",
    "    \n",
    "    def fit(self, X, y, max_epochs=1000, patience=100, verbose=False):\n",
    "        X_scaled = self.scalers['X'].fit_transform(X)\n",
    "        y_scaled = self.scalers['y'].fit_transform(y.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        y_tensor = torch.FloatTensor(y_scaled).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for model_idx, (model, optimizer, scheduler) in enumerate(\n",
    "            zip(self.models, self.optimizers, self.schedulers)\n",
    "        ):\n",
    "            model.train()\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            best_state = None\n",
    "            \n",
    "            for epoch in range(max_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(X_tensor)\n",
    "                loss = criterion(pred, y_tensor)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                current_loss = loss.item()\n",
    "                \n",
    "                if current_loss < best_loss - 1e-6:\n",
    "                    best_loss = current_loss\n",
    "                    patience_counter = 0\n",
    "                    best_state = model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"  Model {model_idx}: Early stop at epoch {epoch}, loss={best_loss:.6f}\")\n",
    "                    break\n",
    "            \n",
    "            if best_state is not None:\n",
    "                model.load_state_dict(best_state)\n",
    "    \n",
    "    def predict(self, X, return_std=True):\n",
    "        X_scaled = self.scalers['X'].transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        \n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_scaled = model(X_tensor).cpu().numpy()\n",
    "                pred = self.scalers['y'].inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        mean = predictions.mean(axis=0)\n",
    "        std = predictions.std(axis=0)\n",
    "        \n",
    "        if return_std:\n",
    "            return mean, std\n",
    "        return mean\n",
    "    \n",
    "    def gradient_at(self, x):\n",
    "        x_scaled = self.scalers['X'].transform(x.reshape(1, -1))\n",
    "        x_tensor = torch.FloatTensor(x_scaled).to(device).requires_grad_(True)\n",
    "        \n",
    "        gradients = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            pred = model(x_tensor)\n",
    "            pred.backward()\n",
    "            grad = x_tensor.grad.cpu().numpy().flatten()\n",
    "            grad = grad / self.scalers['X'].scale_\n",
    "            gradients.append(grad)\n",
    "            x_tensor.grad.zero_()\n",
    "        \n",
    "        return np.mean(gradients, axis=0)\n",
    "\n",
    "\n",
    "print(\"Enhanced Neural Network Surrogate Classes Defined\")\n",
    "print(\"  - Kaiming initialization (He et al.)\")\n",
    "print(\"  - Cosine annealing LR schedule\")\n",
    "print(\"  - Gradient clipping for stability\")\n",
    "print(\"  - Diverse ensemble architectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquisition-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_ucb_acquisition(X, ensemble, kappa=1.96):\n",
    "    \"\"\"Upper Confidence Bound using NN ensemble.\"\"\"\n",
    "    mu, sigma = ensemble.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "\n",
    "def nn_expected_improvement(X, ensemble, y_best, xi=0.01):\n",
    "    \"\"\"Expected Improvement using NN ensemble.\"\"\"\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    mu, sigma = ensemble.predict(X, return_std=True)\n",
    "    \n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - y_best - xi\n",
    "        Z = np.divide(imp, sigma, out=np.zeros_like(imp), where=sigma > 1e-6)\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma < 1e-6] = 0.0\n",
    "    \n",
    "    return ei\n",
    "\n",
    "\n",
    "def gradient_ascent_acquisition(ensemble, start_x, bounds, n_steps=100, lr=0.01):\n",
    "    \"\"\"Gradient ascent on the NN surrogate.\"\"\"\n",
    "    x = start_x.copy()\n",
    "    lower = np.array([b[0] for b in bounds])\n",
    "    upper = np.array([b[1] for b in bounds])\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        grad = ensemble.gradient_at(x)\n",
    "        x = x + lr * grad\n",
    "        x = np.clip(x, lower, upper)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def local_perturbation(best_x, bounds, perturbation_scale=0.05, bias=None):\n",
    "    \"\"\"Generate a point by perturbing the best known point.\"\"\"\n",
    "    dim = len(best_x)\n",
    "    lower = np.array([b[0] for b in bounds])\n",
    "    upper = np.array([b[1] for b in bounds])\n",
    "    ranges = upper - lower\n",
    "    \n",
    "    noise = np.random.normal(0, perturbation_scale, dim) * ranges\n",
    "    new_x = best_x + noise\n",
    "    \n",
    "    if bias is not None:\n",
    "        new_x = new_x + np.array(bias)\n",
    "    \n",
    "    return np.clip(new_x, lower, upper)\n",
    "\n",
    "\n",
    "def format_query(x):\n",
    "    \"\"\"Formats the query point as a string for submission.\"\"\"\n",
    "    return \"-\".join([f\"{val:.6f}\" for val in x])\n",
    "\n",
    "\n",
    "print(\"Acquisition Functions Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-all-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load and analyze all functions with Week 4 results\n",
    "function_data = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA SUMMARY - Week 5 (14 data points per 2D function)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for func_id in range(1, 9):\n",
    "    df = load_data(func_id)\n",
    "    input_cols = [c for c in df.columns if c.startswith('x')]\n",
    "    X = df[input_cols].values\n",
    "    y = df['y'].values\n",
    "    \n",
    "    function_data[func_id] = {\n",
    "        'df': df,\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'dim': len(input_cols),\n",
    "        'y_best': y.max(),\n",
    "        'best_idx': y.argmax(),\n",
    "        'best_x': X[y.argmax()]\n",
    "    }\n",
    "    \n",
    "    # Find best source\n",
    "    best_source = df.iloc[y.argmax()]['source']\n",
    "    \n",
    "    display(Markdown(f\"### Function {func_id} ({len(input_cols)}D)\"))\n",
    "    print(f\"Samples: {len(df)} | Output range: [{y.min():.4f}, {y.max():.4f}]\")\n",
    "    print(f\"Best: y={y.max():.6f} at {format_query(X[y.argmax()])} ({best_source})\")\n",
    "    \n",
    "    # Show Week 4 result\n",
    "    week4_data = df[df['source'] == 'week_4_submission']\n",
    "    if len(week4_data) > 0:\n",
    "        w4_y = week4_data['y'].values[0]\n",
    "        status = \"âœ“ NEW BEST\" if w4_y == y.max() else f\"(best: {y.max():.4f})\"\n",
    "        print(f\"Week 4 result: y = {w4_y:.6f} {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-ensembles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved NN ensembles for each function\n",
    "print(\"Training Enhanced Neural Network Ensembles (Module 16 Architecture)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ensembles = {}\n",
    "\n",
    "for func_id in range(1, 9):\n",
    "    data = function_data[func_id]\n",
    "    dim = data['dim']\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    print(f\"\\nFunction {func_id} ({dim}D, {n_samples} samples):\")\n",
    "    \n",
    "    ensemble = ImprovedEnsembleSurrogate(\n",
    "        input_dim=dim,\n",
    "        n_samples=n_samples,\n",
    "        n_models=5\n",
    "    )\n",
    "    \n",
    "    ensemble.fit(X, y, max_epochs=1000, patience=100, verbose=False)\n",
    "    \n",
    "    ensembles[func_id] = ensemble\n",
    "    \n",
    "    # Evaluate ensemble quality\n",
    "    y_pred, y_std = ensemble.predict(X)\n",
    "    mse = np.mean((y_pred - y) ** 2)\n",
    "    r2 = 1 - mse / np.var(y)\n",
    "    print(f\"  Training RÂ²: {r2:.4f} | Mean uncertainty: {y_std.mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All enhanced ensembles trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "week5-strategy",
   "metadata": {},
   "source": [
    "## Week 5 Strategy: Evidence-Based Refinement\n",
    "\n",
    "### Week 4 Outcomes Analysis\n",
    "\n",
    "| Function | W4 Strategy | W4 Result | W4 vs Best | Week 5 Action |\n",
    "|----------|-------------|-----------|------------|---------------|\n",
    "| **F1** | Grid search | 0.196 | **NEW BEST** ðŸš€ | Exploit around [0.65, 0.65] |\n",
    "| **F2** | Exact return | 0.667 | **NEW BEST** ðŸš€ | Micro-perturb around [0.70, 0.93] |\n",
    "| F3 | NN gradient | -0.085 | Worse | Return to initial best |\n",
    "| F4 | Exploration | -24.52 | Much worse | Return to W1 coordinates |\n",
    "| F5 | NN gradient | 1289.3 | Worse | Return to W1 coordinates |\n",
    "| **F6** | Exact return | -0.717 | Very close | Tiny perturbation |\n",
    "| **F7** | Micro-perturb | 2.396 | **NEW BEST** ðŸš€ | Continue micro-perturb |\n",
    "| F8 | NN gradient | 9.877 | Close | Return to W3 best |\n",
    "\n",
    "### Key Strategic Insights\n",
    "\n",
    "1. **F1 Breakthrough**: The [0.65, 0.65] region is special - explore nearby!\n",
    "2. **Exact Return Works**: F2, F6 benefited from returning to known good points\n",
    "3. **NN Gradients Over-corrected**: F3, F5, F8 all moved away from optima\n",
    "4. **F4 Exploration Failed Badly**: The positive region is very localized\n",
    "\n",
    "### Week 5 Strategy Summary\n",
    "\n",
    "| Function | Strategy | Rationale |\n",
    "|----------|----------|----------|\n",
    "| 1 | **EXPLOIT_DISCOVERY** | Found 0.196 at [0.65, 0.65] - search nearby |\n",
    "| 2 | **MICRO_PERTURB** | W4 beat all previous - tiny refinement |\n",
    "| 3 | **EXACT_RETURN** | Return to initial best (-0.035) |\n",
    "| 4 | **EXACT_RETURN** | Must return to W1's only positive value |\n",
    "| 5 | **EXACT_RETURN** | Return to W1's 1618.5 peak |\n",
    "| 6 | **MICRO_PERTURB** | Very close to best, tiny refinement |\n",
    "| 7 | **GRADIENT_REFINE** | W4 success - continue gradient approach |\n",
    "| 8 | **EXACT_RETURN** | Return to W3's 9.915 peak |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special analysis for F1 - the breakthrough function\n",
    "print(\"=\"*60)\n",
    "print(\"SPECIAL ANALYSIS: Function 1 - Exploiting the Breakthrough\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = function_data[1]\n",
    "ensemble = ensembles[1]\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Find the breakthrough point\n",
    "best_idx = y.argmax()\n",
    "best_x = X[best_idx]\n",
    "best_y = y[best_idx]\n",
    "\n",
    "print(f\"\\nBreakthrough point: {format_query(best_x)}\")\n",
    "print(f\"Value: {best_y:.6f}\")\n",
    "\n",
    "# Analyze gradient at breakthrough point\n",
    "grad = ensemble.gradient_at(best_x)\n",
    "print(f\"\\nGradient at breakthrough: [{grad[0]:.4f}, {grad[1]:.4f}]\")\n",
    "print(f\"Gradient magnitude: {np.linalg.norm(grad):.4f}\")\n",
    "print(f\"Gradient direction suggests: move x0 {'up' if grad[0] > 0 else 'down'}, x1 {'up' if grad[1] > 0 else 'down'}\")\n",
    "\n",
    "# Visualize the neighborhood\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: NN prediction surface around breakthrough\n",
    "res = 40\n",
    "x1_grid = np.linspace(0.5, 0.8, res)\n",
    "x2_grid = np.linspace(0.5, 0.8, res)\n",
    "X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
    "X_grid = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "\n",
    "mu_nn, sigma_nn = ensemble.predict(X_grid)\n",
    "\n",
    "im1 = axes[0].contourf(X1, X2, mu_nn.reshape(res, res), cmap='viridis', levels=20)\n",
    "axes[0].scatter([best_x[0]], [best_x[1]], c='red', s=200, marker='*', edgecolors='white', zorder=5)\n",
    "axes[0].set_title('NN Predicted Mean (zoomed)')\n",
    "axes[0].set_xlabel('x0')\n",
    "axes[0].set_ylabel('x1')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot 2: Uncertainty\n",
    "im2 = axes[1].contourf(X1, X2, sigma_nn.reshape(res, res), cmap='plasma', levels=20)\n",
    "axes[1].scatter([best_x[0]], [best_x[1]], c='red', s=200, marker='*', edgecolors='white', zorder=5)\n",
    "axes[1].set_title('NN Uncertainty (zoomed)')\n",
    "axes[1].set_xlabel('x0')\n",
    "axes[1].set_ylabel('x1')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Plot 3: Sample candidate points\n",
    "np.random.seed(42)\n",
    "candidates = []\n",
    "for _ in range(100):\n",
    "    perturbation = np.random.normal(0, 0.05, 2)\n",
    "    candidate = np.clip(best_x + perturbation, 0.01, 0.99)\n",
    "    candidates.append(candidate)\n",
    "candidates = np.array(candidates)\n",
    "\n",
    "mu_cand, sigma_cand = ensemble.predict(candidates)\n",
    "ucb_cand = mu_cand + 2.0 * sigma_cand\n",
    "\n",
    "im3 = axes[2].scatter(candidates[:, 0], candidates[:, 1], c=ucb_cand, cmap='coolwarm', s=50)\n",
    "axes[2].scatter([best_x[0]], [best_x[1]], c='black', s=200, marker='*', edgecolors='white', zorder=5)\n",
    "axes[2].set_title('UCB at Candidate Points')\n",
    "axes[2].set_xlabel('x0')\n",
    "axes[2].set_ylabel('x1')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best candidate\n",
    "best_cand_idx = ucb_cand.argmax()\n",
    "print(f\"\\nBest candidate point: {format_query(candidates[best_cand_idx])}\")\n",
    "print(f\"UCB value: {ucb_cand[best_cand_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "week5-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Configuration based on Week 4 outcomes\n",
    "WEEK5_CONFIG = {\n",
    "    1: {\n",
    "        'strategy': 'EXPLOIT_DISCOVERY',\n",
    "        'center': [0.65, 0.65],  # Week 4 breakthrough point\n",
    "        'perturb_scale': 0.03,   # Small perturbation around success\n",
    "        'desc': '2D - Exploit breakthrough region (0.196 found at [0.65, 0.65])'\n",
    "    },\n",
    "    2: {\n",
    "        'strategy': 'MICRO_PERTURB',\n",
    "        'target': [0.702637, 0.926564],  # Week 4 best (0.667)\n",
    "        'perturb_scale': 0.02,\n",
    "        'desc': '2D - Micro-perturb W4 best (0.667 beat all previous)'\n",
    "    },\n",
    "    3: {\n",
    "        'strategy': 'EXACT_RETURN',\n",
    "        'target': [0.492581, 0.611593, 0.340176],  # Initial best (-0.035)\n",
    "        'desc': '3D - Return to initial best (-0.035), W4 regression'\n",
    "    },\n",
    "    4: {\n",
    "        'strategy': 'EXACT_RETURN',\n",
    "        'target': [0.404559, 0.414786, 0.357365, 0.399048],  # W1 only positive\n",
    "        'desc': '4D - Return to W1 coordinates (only positive value)'\n",
    "    },\n",
    "    5: {\n",
    "        'strategy': 'EXACT_RETURN',\n",
    "        'target': [0.362718, 0.273413, 0.996088, 0.997538],  # W1 peak\n",
    "        'desc': '4D - Return to W1 peak (1618.5)'\n",
    "    },\n",
    "    6: {\n",
    "        'strategy': 'MICRO_PERTURB',\n",
    "        'target': [0.728186, 0.154693, 0.732552, 0.693997, 0.056401],  # W4 result\n",
    "        'perturb_scale': 0.01,\n",
    "        'desc': '5D - Micro-perturb (W4 very close to best)'\n",
    "    },\n",
    "    7: {\n",
    "        'strategy': 'NN_GRADIENT',\n",
    "        'start': [0.037482, 0.151519, 0.584151, 0.238672, 0.376091, 0.784901],  # W4 best\n",
    "        'lr': 0.005,  # Conservative learning rate\n",
    "        'desc': '6D - Gradient refinement from W4 best (2.396)'\n",
    "    },\n",
    "    8: {\n",
    "        'strategy': 'EXACT_RETURN',\n",
    "        'target': [0.024511, 0.095108, 0.162460, 0.036406, 0.886768, 0.318315, 0.166845, 0.204731],  # W3 best\n",
    "        'desc': '8D - Return to W3 best (9.915)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate Week 5 queries\n",
    "week5_queries = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WEEK 5 QUERY GENERATION - Evidence-Based Strategy\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for func_id, config in WEEK5_CONFIG.items():\n",
    "    display(Markdown(f\"### Function {func_id}: {config['desc']}\"))\n",
    "    \n",
    "    data = function_data[func_id]\n",
    "    ensemble = ensembles[func_id]\n",
    "    dim = data['dim']\n",
    "    bounds = [(0.01, 0.99)] * dim\n",
    "    y_best = data['y_best']\n",
    "    \n",
    "    strategy = config['strategy']\n",
    "    np.random.seed(42 + func_id)  # Reproducible randomness\n",
    "    \n",
    "    if strategy == 'EXACT_RETURN':\n",
    "        next_x = np.array(config['target'])\n",
    "        print(f\"Strategy: EXACT RETURN to proven coordinates\")\n",
    "        pred_mu, pred_std = ensemble.predict(next_x.reshape(1, -1))\n",
    "        print(f\"NN prediction at target: y = {pred_mu[0]:.4f} Â± {pred_std[0]:.4f}\")\n",
    "        \n",
    "    elif strategy == 'EXPLOIT_DISCOVERY':\n",
    "        center = np.array(config['center'])\n",
    "        scale = config['perturb_scale']\n",
    "        \n",
    "        # Use gradient to inform direction\n",
    "        grad = ensemble.gradient_at(center)\n",
    "        grad_normalized = grad / (np.linalg.norm(grad) + 1e-8)\n",
    "        \n",
    "        # Generate candidates around center, biased by gradient\n",
    "        best_ucb = -np.inf\n",
    "        best_candidate = center\n",
    "        \n",
    "        for _ in range(100):\n",
    "            noise = np.random.normal(0, scale, dim)\n",
    "            # Add small gradient bias\n",
    "            candidate = center + noise + 0.01 * grad_normalized\n",
    "            candidate = np.clip(candidate, 0.01, 0.99)\n",
    "            \n",
    "            ucb = nn_ucb_acquisition(candidate.reshape(1, -1), ensemble, kappa=2.0)[0]\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_candidate = candidate\n",
    "        \n",
    "        next_x = best_candidate\n",
    "        print(f\"Strategy: EXPLOIT DISCOVERY around breakthrough\")\n",
    "        print(f\"Gradient direction: [{grad[0]:.4f}, {grad[1]:.4f}]\")\n",
    "        pred_mu, pred_std = ensemble.predict(next_x.reshape(1, -1))\n",
    "        print(f\"Best candidate UCB: {best_ucb:.4f}, predicted: {pred_mu[0]:.4f}\")\n",
    "        \n",
    "    elif strategy == 'MICRO_PERTURB':\n",
    "        target = np.array(config['target'])\n",
    "        scale = config.get('perturb_scale', 0.02)\n",
    "        \n",
    "        # Use gradient to inform perturbation direction\n",
    "        grad = ensemble.gradient_at(target)\n",
    "        grad_normalized = grad / (np.linalg.norm(grad) + 1e-8)\n",
    "        \n",
    "        noise = np.random.normal(0, scale, dim)\n",
    "        next_x = target + noise + 0.005 * grad_normalized\n",
    "        next_x = np.clip(next_x, 0.01, 0.99)\n",
    "        \n",
    "        print(f\"Strategy: MICRO PERTURBATION (scale={scale})\")\n",
    "        print(f\"Gradient direction: {['%.4f' % g for g in grad]}\")\n",
    "        pred_mu, pred_std = ensemble.predict(next_x.reshape(1, -1))\n",
    "        print(f\"Predicted: y = {pred_mu[0]:.4f} Â± {pred_std[0]:.4f}\")\n",
    "        \n",
    "    elif strategy == 'NN_GRADIENT':\n",
    "        start_x = np.array(config['start'])\n",
    "        lr = config.get('lr', 0.01)\n",
    "        \n",
    "        next_x = gradient_ascent_acquisition(\n",
    "            ensemble, start_x, bounds, n_steps=50, lr=lr\n",
    "        )\n",
    "        \n",
    "        print(f\"Strategy: NN GRADIENT ASCENT (lr={lr})\")\n",
    "        grad = ensemble.gradient_at(start_x)\n",
    "        print(f\"Initial gradient: {['%.4f' % g for g in grad]}\")\n",
    "        pred_mu, pred_std = ensemble.predict(next_x.reshape(1, -1))\n",
    "        print(f\"Predicted after gradient ascent: y = {pred_mu[0]:.4f}\")\n",
    "    \n",
    "    # Store and format query\n",
    "    query_str = format_query(next_x)\n",
    "    week5_queries[func_id] = {'query': query_str, 'array': next_x}\n",
    "    \n",
    "    print(f\"\\n>>> Week 5 Query: {query_str}\")\n",
    "    \n",
    "    # Save submission\n",
    "    save_submission(func_id, query_str, module_name=\"Module 16 - Week 5\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"WEEK 5 QUERY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for func_id in range(1, 9):\n",
    "    query = week5_queries[func_id]['query']\n",
    "    strategy = WEEK5_CONFIG[func_id]['strategy']\n",
    "    print(f\"\\nFunction {func_id} ({strategy}):\")\n",
    "    print(f\"  {query}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All queries saved to submissions/submission_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEEK 5 STRATEGY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n{:<8} {:<4} {:<20} {:<12} {:<12}\".format(\n",
    "    \"Function\", \"Dim\", \"Strategy\", \"Week 4 y\", \"Best Ever\"\n",
    "))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "week4_results = {\n",
    "    1: 0.196, 2: 0.667, 3: -0.085, 4: -24.52,\n",
    "    5: 1289.34, 6: -0.717, 7: 2.396, 8: 9.877\n",
    "}\n",
    "\n",
    "for func_id in range(1, 9):\n",
    "    dim = function_data[func_id]['dim']\n",
    "    strategy = WEEK5_CONFIG[func_id]['strategy']\n",
    "    w4_y = week4_results[func_id]\n",
    "    best_y = function_data[func_id]['y_best']\n",
    "    \n",
    "    print(\"{:<8} {:<4} {:<20} {:<12.4f} {:<12.4f}\".format(\n",
    "        f\"F{func_id}\", f\"{dim}D\", strategy, w4_y, best_y\n",
    "    ))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection: Module 16 Concepts Applied to BBO\n",
    "\n",
    "### 1. How do AlexNet's architectural innovations relate to our surrogate design?\n",
    "\n",
    "**AlexNet's Key Innovations (2012)**:\n",
    "- ReLU activation (fast, avoids vanishing gradients)\n",
    "- Dropout regularization (prevents overfitting)\n",
    "- GPU training (enables larger models)\n",
    "- Data augmentation (more effective use of limited data)\n",
    "\n",
    "**Application to our NN Surrogates**:\n",
    "- **ReLU**: We use ReLU in all hidden layers for efficient gradient flow\n",
    "- **Dropout**: Essential with only 14-44 samples; we use adaptive dropout (higher for smaller datasets)\n",
    "- **LayerNorm**: We use LayerNorm instead of BatchNorm (works with batch size 1 for single-point prediction)\n",
    "- **Ensemble**: Like data augmentation, using 5 diverse models provides implicit regularization\n",
    "\n",
    "### 2. How does the concept of feature hierarchies apply to BBO?\n",
    "\n",
    "In image classification:\n",
    "- Layer 1: Detects edges and textures\n",
    "- Layer 2: Combines into shapes\n",
    "- Layer 3+: Recognizes objects\n",
    "\n",
    "**In our BBO surrogates**:\n",
    "- **Input layer**: Raw function coordinates\n",
    "- **Hidden layer 1**: Learns which dimensions correlate with output (e.g., xâ‚‚, xâ‚ƒ for F5)\n",
    "- **Hidden layer 2**: Combines dimension interactions\n",
    "- **Output**: Predicts function value\n",
    "\n",
    "Evidence: For Function 5, gradients showed xâ‚‚ and xâ‚ƒ have strong positive effects, while xâ‚€ and xâ‚ are nearly neutral. The network learned this hierarchy automatically.\n",
    "\n",
    "### 3. How do PyTorch's design choices benefit our approach?\n",
    "\n",
    "**Dynamic Computation Graphs**:\n",
    "- We can compute gradients at any arbitrary point x using `backward()`\n",
    "- Essential for gradient-based acquisition functions\n",
    "- Would be harder with TensorFlow's static graph (pre-2.0)\n",
    "\n",
    "**Autograd**:\n",
    "- `requires_grad_(True)` lets us compute âˆ‚y/âˆ‚x without manual differentiation\n",
    "- The `gradient_at()` method leverages this directly\n",
    "\n",
    "**Module API**:\n",
    "- Clean separation between architecture (`SurrogateNN`) and training (`fit()`)\n",
    "- Easy to create diverse ensemble members\n",
    "\n",
    "### 4. What are the five building blocks in our surrogate model?\n",
    "\n",
    "| Block | Implementation | Purpose |\n",
    "|-------|----------------|--------|\n",
    "| **Linear** | `nn.Linear(prev_dim, hidden_dim)` | Transform input dimensions |\n",
    "| **Activation** | `nn.ReLU()` | Non-linear function approximation |\n",
    "| **Normalization** | `nn.LayerNorm(hidden_dim)` | Stabilize training, works with batch=1 |\n",
    "| **Regularization** | `nn.Dropout(0.1-0.3)` | Prevent overfitting on small data |\n",
    "| **Loss** | `nn.MSELoss()` | Regression objective |\n",
    "\n",
    "### 5. How do architectural trade-offs in deep learning parallel BBO strategy?\n",
    "\n",
    "**Depth vs Width**:\n",
    "- Deeper networks: More expressive but harder to train (vanishing gradients)\n",
    "- Wider networks: More parameters per layer but may overfit\n",
    "- **BBO parallel**: Exploration (deep search) vs Exploitation (wide local search)\n",
    "\n",
    "**Regularization Strength**:\n",
    "- Too much: Underfitting (model too conservative)\n",
    "- Too little: Overfitting (model memorizes noise)\n",
    "- **BBO parallel**: Too much exploration wastes queries; too much exploitation misses better regions\n",
    "\n",
    "**Learning Rate**:\n",
    "- Too high: Training diverges\n",
    "- Too low: Training too slow\n",
    "- **BBO parallel**: Perturbation scale - too large jumps miss optima, too small gets stuck\n",
    "\n",
    "### 6. Week 5 strategy informed by Module 16\n",
    "\n",
    "**Evidence-based fine-tuning**:\n",
    "- Like fine-tuning a pre-trained model, we start from known good points\n",
    "- Use small learning rates (perturbation scales) to avoid destroying learned knowledge\n",
    "- Function 1's breakthrough is like finding a good initialization - now we fine-tune\n",
    "\n",
    "**Ensemble diversity**:\n",
    "- Different architectures in our ensemble = implicit regularization\n",
    "- Uncertainty from disagreement guides exploration\n",
    "- Similar to how ImageNet models benefit from ensemble predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-part2",
   "metadata": {},
   "source": [
    "## Part 2: Strategy Reflection Questions\n",
    "\n",
    "### 1. How did Week 4 results inform your Week 5 strategy?\n",
    "\n",
    "**Key Lessons**:\n",
    "\n",
    "1. **Function 1 Breakthrough**: The grid search at [0.65, 0.65] found a non-zero value (0.196) after 4 weeks of near-zero results. This is the most important discovery of the project. Week 5 must exploit this finding.\n",
    "\n",
    "2. **Exact Return Works**: Functions 2 and 6 showed that returning to the best known coordinates can yield even better results. F2 improved from 0.611 (initial best) to 0.667.\n",
    "\n",
    "3. **Gradient Methods Over-corrected**: Functions 3, 5, and 8 all regressed when using NN gradient ascent. The gradients pointed in wrong directions. For Week 5, we return to exact coordinates for these functions.\n",
    "\n",
    "4. **F4 Exploration Failed**: Moving to [0.75, 0.75, 0.75, 0.75] gave -24.52, proving the positive region is NOT in the high corner. The Week 1 point remains the only positive value.\n",
    "\n",
    "### 2. Which functions benefited most from neural network surrogates?\n",
    "\n",
    "**Clear Benefits**:\n",
    "- **F7**: NN micro-perturbation improved from 2.29 to 2.396 (new best)\n",
    "- **F1**: NN uncertainty helped identify the [0.65, 0.65] region as worth exploring\n",
    "\n",
    "**Mixed Results**:\n",
    "- **F3, F5, F8**: NN gradients pointed in suboptimal directions\n",
    "\n",
    "**Key Insight**: NNs work best for small refinements around known good points, not for large jumps.\n",
    "\n",
    "### 3. How does the \"exploration vs exploitation\" trade-off manifest in deep learning concepts?\n",
    "\n",
    "| Deep Learning | BBO Equivalent |\n",
    "|---------------|----------------|\n",
    "| High learning rate | Large perturbations (exploration) |\n",
    "| Low learning rate | Small perturbations (exploitation) |\n",
    "| Random initialization | Initial sampling (exploration) |\n",
    "| Fine-tuning | Local refinement (exploitation) |\n",
    "| Dropout | Stochastic exploration |\n",
    "| Early stopping | Stopping when no improvement |\n",
    "\n",
    "### 4. What would you do differently with more queries?\n",
    "\n",
    "With unlimited queries:\n",
    "1. **F1**: Dense grid search around [0.65, 0.65] to map the peak\n",
    "2. **F4**: Systematic exploration of all corners to find the positive region's extent\n",
    "3. **F5**: Latin Hypercube Sampling to better characterize the response surface\n",
    "4. **All**: Bayesian optimization with Thompson Sampling for principled exploration\n",
    "\n",
    "### 5. Final confidence levels for Week 5 queries\n",
    "\n",
    "| Function | Confidence | Rationale |\n",
    "|----------|------------|----------|\n",
    "| F1 | **High** | Exploiting proven success at [0.65, 0.65] |\n",
    "| F2 | **High** | Micro-perturbing W4's new best (0.667) |\n",
    "| F3 | **Medium** | Returning to initial best, should be safe |\n",
    "| F4 | **High** | Returning to only positive value |\n",
    "| F5 | **High** | Returning to W1's proven best (1618.5) |\n",
    "| F6 | **Medium** | Micro-perturbing near-optimal point |\n",
    "| F7 | **Medium** | Gradient refinement from new best |\n",
    "| F8 | **High** | Returning to W3's proven best (9.915) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wmw0yi25ih",
   "source": "---\n\n## Part 3: Critical Reflection on Week 5 Strategy\n\n### 1. How did the ideas of hierarchical feature learning influence the way you thought about structuring or refining your optimisation strategy this round?\n\nHierarchical feature learning fundamentally changed how I approached the BBO problem this week. In deep learning, early layers learn simple features (edges, textures) that combine into increasingly complex representations. I applied this thinking to my optimization strategy in three ways:\n\n**Layered Strategy Design**:\n- **Layer 1 (Data)**: Raw function evaluations â†’ identify which dimensions correlate with output\n- **Layer 2 (Patterns)**: Combine dimension insights â†’ recognize regional behavior (e.g., F5's high-corner optimum)\n- **Layer 3 (Strategy)**: Synthesize into function-specific approaches â†’ EXACT_RETURN vs GRADIENT_REFINE\n\n**Dimension Importance Hierarchy**:\nFor Function 5, the NN learned that xâ‚‚ and xâ‚ƒ are \"high-level features\" (strong gradients) while xâ‚€ and xâ‚ are \"low-level\" (weak influence). This informed my decision to keep xâ‚‚, xâ‚ƒ at boundary values while being flexible with xâ‚€, xâ‚.\n\n**Progressive Refinement**:\nLike a CNN building from edges to objects, my strategy evolved from broad exploration (Week 1) â†’ regional targeting (Week 3) â†’ fine-tuning around discoveries (Week 5). The F1 breakthrough at [0.65, 0.65] is analogous to finding the right \"feature\" that the network can now refine.\n\n---\n\n### 2. What parallels do you see between breakthroughs like AlexNet/ImageNet and the incremental improvements in your capstone submissions?\n\n**The AlexNet Moment**: In 2012, AlexNet didn't just win ImageNetâ€”it shattered expectations by reducing error from 26% to 15%. This wasn't incremental; it was a paradigm shift enabled by three factors: **architecture** (deep CNNs), **data** (ImageNet scale), and **compute** (GPUs).\n\n**Parallels in My BBO Journey**:\n\n| AlexNet Breakthrough | BBO Capstone Parallel |\n|---------------------|----------------------|\n| ReLU enabling deep training | NN surrogates enabling gradient-guided search |\n| Dropout preventing overfitting | Ensemble uncertainty preventing over-exploitation |\n| ImageNet's scale | Accumulated 14+ samples per function |\n| GPU acceleration | PyTorch's efficient autograd |\n\n**My \"AlexNet Moment\"**: Function 1's breakthrough in Week 4 (0.196 after 4 weeks of zeros) parallels AlexNet's leap. It wasn't incremental improvementâ€”it was finding the right region after systematic search. This single discovery changed my entire Week 5 strategy from exploration to exploitation.\n\n**The Incremental Reality**: However, most of my improvements are incremental (F7: 2.29 â†’ 2.396, a 4.6% gain). This mirrors post-AlexNet progressâ€”VGGNet, ResNet, etc. made important but smaller gains. The lesson: breakthroughs are rare, but they redefine what's possible. My F1 breakthrough justifies continued investment in that region.\n\n---\n\n### 3. Did you encounter trade-offs between depth, complexity, and training efficiency similar to those in neural network design?\n\n**Yes, directly analogous trade-offs emerged**:\n\n**Depth vs Efficiency (Exploration vs Query Budget)**:\n- **Deep exploration**: Systematic grid search of entire space would find global optimum but requires hundreds of queries\n- **Shallow exploitation**: Local perturbation is efficient but may miss better regions\n- **My compromise**: Adaptive depthâ€”deeper search for functions with unclear optima (F4), shallow refinement for functions with known peaks (F5, F8)\n\n**Complexity vs Overfitting (Model Trust vs Data Scarcity)**:\n- **Complex NN**: Can model intricate response surfaces but may \"hallucinate\" patterns from 14 samples\n- **Simple linear**: Won't overfit but misses non-linearities\n- **My compromise**: Ensemble of diverse architecturesâ€”complexity averaged across models reduces overfitting while maintaining flexibility\n\n**Training Efficiency Trade-off (Query Efficiency)**:\n| Deep Learning | BBO Equivalent | My Week 5 Choice |\n|---------------|----------------|------------------|\n| Large batch size | Many parallel queries | Not available (1 query/week) |\n| Learning rate scheduling | Adaptive perturbation scale | Start small (0.01-0.03), reduce over weeks |\n| Early stopping | Stop exploring when confident | EXACT_RETURN for F4, F5, F8 |\n\n**Critical Trade-off Decision**: For F4, I chose safety over potential gain. The NN predicted -28.5 at [0.75, 0.75, 0.75, 0.75], which proved correct (-24.52). Rather than risk another bad query, I'm returning to the only positive pointâ€”sacrificing exploration depth for exploitation efficiency.\n\n---\n\n### 4. Which neural network building blocks helped you think differently about how your model learns from accumulated data?\n\n**Gradients (âˆ‚L/âˆ‚w â†’ âˆ‚y/âˆ‚x)**:\nThe most transformative concept. In neural networks, gradients tell us how to update weights. I applied this directly: **gradients of the surrogate tell us how to update query positions**. The `gradient_at()` method computes âˆ‚y/âˆ‚x, revealing which direction improves the predicted output.\n\nFor F7, the gradient at the Week 4 best point showed:\n```\n[âˆ’0.15, +0.02, âˆ’0.08, +0.03, +0.05, âˆ’0.04]\n```\nThis told me: decrease xâ‚€, xâ‚‚; increase xâ‚, xâ‚ƒ, xâ‚„. The Week 5 query follows this direction with conservative step size.\n\n**Loss Functions (MSE â†’ Acquisition Functions)**:\nIn NNs, the loss function defines what \"good\" means. I realized acquisition functions (UCB, EI) are analogousâ€”they define what makes a query \"good.\" UCB = Î¼ + ÎºÏƒ balances predicted value (exploitation) with uncertainty (exploration), just as regularized loss balances fit and generalization.\n\n**Weight Initialization â†’ Starting Point Selection**:\nBad initialization can doom NN training. Similarly, choosing the right starting point for gradient ascent is critical. For F7, starting from the Week 4 best (2.396) rather than a random point leverages accumulated knowledgeâ€”like transfer learning.\n\n**Activations (ReLU â†’ Response Surface Shape)**:\nReLU creates piecewise-linear decision boundaries. This made me realize my surrogate learns a piecewise approximation of the true function. The \"kinks\" in the learned surface might not match the true function's shape, explaining why gradient methods over-corrected for F3, F5, F8.\n\n---\n\n### 5. Would your current optimisation approach be closer to rapid prototyping/flexibility or structured/production-ready design?\n\n**Firmly in the \"Rapid Prototyping\" Paradigm** (PyTorch-style):\n\n| Characteristic | PyTorch (Research) | TensorFlow (Production) | My BBO Approach |\n|----------------|-------------------|------------------------|-----------------|\n| **Iteration speed** | Fast experimentation | Careful deployment | Weekly pivots based on results |\n| **Flexibility** | Change architecture mid-training | Static graph optimization | Function-specific strategies |\n| **Debugging** | Easy inspection | Harder to trace | Direct visualization of NN predictions |\n| **Reproducibility** | Set seeds, hope for best | Deterministic by design | Seeds set, but ensemble variance exists |\n\n**Why Rapid Prototyping Fits**:\n\n1. **Limited Data**: With only 14 samples per 2D function, I can't do production-style hyperparameter tuning. I need flexible architectures that adapt to data size.\n\n2. **Weekly Feedback**: Each week's results inform next week's strategy. This is research-style iteration, not deployment.\n\n3. **Uncertainty is Feature**: In production, you want confident predictions. Here, uncertainty (ensemble disagreement) actively guides exploration.\n\n4. **Dynamic Strategy**: My approach changes per-function and per-week. F1 went from ADAPTIVE_GRID (Week 4) to EXPLOIT_DISCOVERY (Week 5). A production system would have fixed pipelines.\n\n**What Would Make It Production-Ready**:\n- Fixed architecture per function class (2D, 4D, 8D)\n- Automated hyperparameter selection\n- Guaranteed convergence bounds\n- Deployment-ready query generation API\n\nFor now, flexibility > structure because the problem is still being understood.\n\n---\n\n### 6. How might reflecting on real-world deep learning use cases (like Giovanni Liotta's sports applications) inform how you benchmark success?\n\n**Giovanni Liotta's Sports Analytics Insights**:\nIn the interview, Liotta discussed how deep learning transforms sports through:\n- **Player tracking**: Real-time position data â†’ tactical insights\n- **Performance prediction**: Historical data â†’ injury risk, peak performance timing\n- **Strategy optimization**: Game theory + ML â†’ optimal play calling\n\n**Parallels to BBO Benchmarking**:\n\n| Sports Analytics | BBO Capstone |\n|------------------|--------------|\n| Win/loss is ultimate metric | Best function value is ultimate metric |\n| But process metrics matter (possession %, shot accuracy) | But learning metrics matter (RÂ², uncertainty reduction) |\n| Single game variance is high | Single query variance is high |\n| Season-long trends reveal truth | Multi-week trends reveal truth |\n\n**Revised Success Benchmarks**:\n\n1. **Outcome Metrics** (like wins):\n   - Primary: Did we beat or match best-ever value?\n   - F1: 0.196 â†’ ðŸŽ¯ (new best)\n   - F4: 0.600 target â†’ need to recover\n\n2. **Process Metrics** (like possession %):\n   - Surrogate RÂ²: Are we learning the function well?\n   - Uncertainty reduction: Are we reducing epistemic uncertainty?\n   - Gradient reliability: Do gradients point toward improvement?\n\n3. **Learning Metrics** (like player development):\n   - Did this week's result teach us something?\n   - F4's -24.52 was a \"loss\" but taught us the high corner is badâ€”valuable information.\n\n**Key Insight from Sports Analytics**:\nLiotta emphasized that **short-term results can be noisy, but long-term strategy wins championships**. Applied to BBO: Week 4's F4 failure (-24.52) isn't just a bad resultâ€”it's information that narrows the search space. A sports team doesn't abandon a strategy after one loss; similarly, I shouldn't abandon exploration entirely after one bad query.\n\n**Benchmark Evolution**:\n- Weeks 1-2: Exploration success (coverage of space)\n- Weeks 3-4: Exploitation success (improvement over best)\n- Week 5: **Consolidation success** (protecting gains while seeking marginal improvements)\n\nThis is like a sports season: early games for experimentation, mid-season for building leads, late season for protecting position while staying competitive.\n\n---\n\n### Summary: Key Takeaways from Module 16 Reflection\n\n1. **Hierarchical thinking** transformed my strategy from flat search to layered refinement\n2. **AlexNet's breakthrough** parallels my F1 discoveryâ€”both enable new strategic possibilities\n3. **Depth vs efficiency trade-offs** directly map to exploration vs exploitation decisions\n4. **Gradients are the key insight**â€”they tell us how to improve, not just what's good\n5. **Rapid prototyping mindset** is appropriate for research-phase optimization\n6. **Sports analytics benchmarking** reminds us that process metrics matter alongside outcomes\n\nThe neural network isn't just a surrogate modelâ€”it's a **thinking framework** for approaching optimization.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}